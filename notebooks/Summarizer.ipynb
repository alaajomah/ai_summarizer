{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-ai-inference\n",
        "\n",
        "import os\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfmk6szAzlFu",
        "outputId": "499970b7-1ce1-4d58-fae0-61c5c92c465c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure-ai-inference in /usr/local/lib/python3.12/dist-packages (1.0.0b9)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from azure-ai-inference) (0.7.2)\n",
            "Requirement already satisfied: azure-core>=1.30.0 in /usr/local/lib/python3.12/dist-packages (from azure-ai-inference) (1.36.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from azure-ai-inference) (4.15.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from azure-core>=1.30.0->azure-ai-inference) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-inference) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Config\n"
      ],
      "metadata": {
        "id": "QiYKKJojz3ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = \"https://models.github.ai/inference\"\n",
        "model = \"meta/Llama-4-Scout-17B-16E-Instruct\"\n",
        "\n",
        "# Make sure GITHUB_TOKEN is defined in your environment / Colab\n",
        "from getpass import getpass\n",
        "token = getpass(\"My GItHub PAT: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUHpSdvkzzO5",
        "outputId": "6ab48aa9-a1de-4f50-e10b-cc676ff9843c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "My GItHub PAT: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Create client"
      ],
      "metadata": {
        "id": "XdJI7fAl0A0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = ChatCompletionsClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(token),\n",
        ")\n"
      ],
      "metadata": {
        "id": "4I4jogzS0EYZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Paragraph summarization"
      ],
      "metadata": {
        "id": "FDDEEKb50pze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"\n",
        "Generative AI, sometimes called gen AI, is artificial intelligence (AI) that can create original content such as text, images, video, audio or software code in response to a user’s prompt or request.\n",
        "\n",
        "Generative AI relies on sophisticated machine learning models called deep learning models algorithms that simulate the learning and decision-making processes of the human brain. These models work by identifying and encoding the patterns and relationships in huge amounts of data, and then using that information to understand users' natural language requests or questions and respond with relevant new content.\n",
        "\n",
        "AI has been a hot technology topic for the past decade, but generative AI, and specifically the arrival of ChatGPT in 2022, has thrust AI into worldwide headlines and launched an unprecedented surge of AI innovation and adoption. Generative AI offers enormous productivity benefits for individuals and organizations, and while it also presents very real challenges and risks, businesses are forging ahead, exploring how the technology can improve their internal workflows and enrich their products and services. According to research by the management consulting firm McKinsey, one third of organizations are already using generative AI regularly in at least one business function.¹ Industry analyst Gartner projects more than 80% of organizations will have deployed generative AI applications or used generative AI application programming interfaces (APIs) by 2026.2\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following paragraph into **exactly 3 concise bullet points**.\n",
        "Rules:\n",
        "- Output ONLY the 3 bullet points.\n",
        "- Use short, clear sentences.\n",
        "- No introduction or conclusion.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{paragraph}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "response = client.complete(\n",
        "    messages=[\n",
        "        SystemMessage(\"You are a professional summarization assistant.\"),\n",
        "        UserMessage(prompt),\n",
        "    ],\n",
        "    temperature=0.2,   # low temp for stable summarization\n",
        "    top_p=0.9,\n",
        "    max_tokens=300,\n",
        "    model=model,\n",
        ")\n",
        "\n",
        "summary = response.choices[0].message.content\n",
        "print(\"Summary:\")\n",
        "print(summary)\n",
        "\n",
        "# (Optional) Save to file for the outputs/ folder\n",
        "with open(\"summary_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0y-JRY20Y6f",
        "outputId": "46dc7126-6416-4139-dac9-9aa7191dd356"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            "• Generative AI creates original content like text, images, and code in response to user prompts.\n",
            "• It relies on deep learning models that simulate human brain processes to identify patterns in data and generate relevant content.\n",
            "• Generative AI is being rapidly adopted, with 1/3 of organizations already using it and 80% expected to deploy it by 2026.\n"
          ]
        }
      ]
    }
  ]
}